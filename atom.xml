<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>littleexplorer</title>
  
  <subtitle>假如人生是一本书，你的书有多厚？</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://smartchar.cn/"/>
  <updated>2020-04-23T09:29:49.740Z</updated>
  <id>https://smartchar.cn/</id>
  
  <author>
    <name>CAI</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>朴实无华枯燥生活 - 英语写作好帮手</title>
    <link href="https://smartchar.cn/2020/04/23/%E6%9C%B4%E5%AE%9E%E6%97%A0%E5%8D%8E%E6%9E%AF%E7%87%A5%E7%94%9F%E6%B4%BB%20-%20%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C%E5%A5%BD%E5%B8%AE%E6%89%8B/"/>
    <id>https://smartchar.cn/2020/04/23/朴实无华枯燥生活 - 英语写作好帮手/</id>
    <published>2020-04-23T08:00:00.000Z</published>
    <updated>2020-04-23T09:29:49.740Z</updated>
    
    <content type="html"><![CDATA[<h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul><li><p>Grammarly, PowerWritingAid and Microsoft Word</p>  <a id="more"></a><p>  少数派的文章<a href="https://sspai.com/post/59984" target="_blank" rel="noopener">我的英语智能写作助手，是否真的「智能」？</a>介绍PowerWritingAid和Grammarly的使用体验对比，正好今天需要检查一下论文的摘要，我就试用了一下这三个软件的效果：Grammarly免费版, PowerWritingAid免费版，Microsoft Word。三个软件都能做的有单词拼写检查、标点使用检查，PowerWritingAid免费版可以在这个基础上进行句式分析，比如检查出被动句并建议优化成主动语态，还可以检查副词并建议删除。显然这些建议不符合科技文的表达风格，于是这个功能对我来说没有什么用处。PowerWritingAid免费版能做到的Grammarly免费版也可以做到，此外Grammarly免费版可以检查定冠词使用，比如缺定冠词或者定冠词用错。<br>  <img src="http://q8b0crcx6.bkt.clouddn.com/article.png" width="100%" height="100%" alt="定冠词检查" align="center"></p>  <center><font size="1">缺少定冠词</font> </center><p>  <img src="http://q8b0crcx6.bkt.clouddn.com/a-an.png" width="100%" height="100%" alt="定冠词检查" align="center"></p>  <center><font size="1">定冠词搭配不当</font> </center></li><li><p>PowerToys<br>  微软官方的提升Windows使用体验的小工具软件合集<a href="https://github.com/microsoft/PowerToys" target="_blank" rel="noopener">PowerToys</a>，功能有文件批量重命名、图像修改尺寸，我觉得比较有趣的是快捷键导航和窗口自动布局功能。快捷键导航是长按<code>Windows</code>键，屏幕会弹出快捷键组合。我Windows组合快捷键使用比较频繁，但用这个功能来了解一些冷门的快捷键很不错。窗口自动布局功能是预先设置一个窗口布局，比如左右分栏。之后按住<code>Shift</code>键将当前窗口拖进分配好的区域即可。下面贴一个快捷键指导的使用过程截图。<br>  <img src="http://q8b0crcx6.bkt.clouddn.com/powertoys.png" width="100%" height="100%" alt="PowerToys" align="center"></p>  <center><font size="1">快捷键导航</font> </center></li></ul><h4 id="散步"><a href="#散步" class="headerlink" title="散步"></a>散步</h4><p><img src="http://q8b0crcx6.bkt.clouddn.com/matou.jpg" width="100%" height="100%" alt="中华路码头" align="center"></p><center><font size="1">中华路码头，乘客乘坐前需要扫码、量体温</font> </center><p><img src="http://q8b0crcx6.bkt.clouddn.com/zhijing.jpg" width="100%" height="100%" alt="致敬白衣天使" align="center"></p><center><font size="1">「致敬白衣天使」</font> </center><h4 id="看剧"><a href="#看剧" class="headerlink" title="看剧"></a>看剧</h4><p>阿尔兹海默症是这个家庭和解的唯一出路。</p><p>39集之前，我，作为一个观众，看的我人都要窒息了。苏家除了与苏家断绝关系的明玉尚存理智，其他人说是心智不全也不过分。苏家大哥虽然人不在国内，但是依然不妨碍他对他弟弟、妹妹以及妻子义正严辞地表达「我对你真的很失望」。二哥享受了父母的各种资助，但视之理所当然，并无多少感恩之心。而且性格冲动，嘴巴里跟叼了炸药一样，跟他爹和妹妹两句话不合人就炸了。苏大强可以说是万恶之源了，自私自利，擅长撒泼耍赖，被苏母「压迫」了几十年的苏大强可怜又可厌。</p><p>39集之后，终于开始出现一点家庭的温情了。就像触底反弹一样，苏家大哥辞职继续待业美国家中，二哥离婚离职前往非洲，苏大强卖房，苏家情况个个落魄、情况不能更糟糕之后，一家人的关系开始缓和。我觉得苏大强这种人跟子女的关系是无法调和的，除非…他患上阿尔兹海默症。这是这个剧想要获得Happy Ending的唯一出路。</p><p>苏家二哥这个角色最真实和饱满，一方面他不怎么懂孝敬和感恩父母，一方面他又十分听老婆的话、疼老婆。明玉的形象太过完美以至于不真实了，做事有分寸、说话有套路、尊师敬友、敬老爱亲。小石吧，没啥缺点，就是这个演员形象总觉得很闰土，配不上高贵大气的明玉吧。<br>1<br><img src="http://q8b0crcx6.bkt.clouddn.com/doutinghao-mingyu.jpg" width="100%" height="100%" alt="苏明玉" align="center"><br><img src="http://q8b0crcx6.bkt.clouddn.com/doutinghao-sudaqiang.jpg" width="100%" height="100%" alt="苏大强" align="center"><br><img src="http://q8b0crcx6.bkt.clouddn.com/doutinghao-mingcheng.jpg" width="100%" height="100%" alt="苏明成" align="center"></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;工具&quot;&gt;&lt;a href=&quot;#工具&quot; class=&quot;headerlink&quot; title=&quot;工具&quot;&gt;&lt;/a&gt;工具&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Grammarly, PowerWritingAid and Microsoft Word&lt;/p&gt;
    
    </summary>
    
    
      <category term="Live" scheme="https://smartchar.cn/tags/Live/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch | 使用GPU进行模型训练</title>
    <link href="https://smartchar.cn/2020/04/18/PyTorch%20-%20%E4%BD%BF%E7%94%A8GPU%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    <id>https://smartchar.cn/2020/04/18/PyTorch - 使用GPU进行模型训练/</id>
    <published>2020-04-18T12:01:44.000Z</published>
    <updated>2020-04-19T03:09:08.177Z</updated>
    
    <content type="html"><![CDATA[<p>使用NVIDIA GPU进行运算，与使用CPU计算相比，算法代码需要进行改动的有<br><a id="more"></a></p><ul><li>模型的输入数据需要转换成GPU支持的数据类型（使用<code>.to(device)</code>）</li><li>模型需要转换成GPU支持的数据类型（使用<code>.to(device)</code>）</li><li>代价函数转换成GPU支持的数据类型（使用<code>.to(device)</code>）</li></ul><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#%% 1.Loading and normalizing CIFAR10</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>,</span><br><span class="line">                                        download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>,</span><br><span class="line">                                       download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="keyword">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># show some of the training images</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br><span class="line">print(<span class="string">"Data Imported."</span>)</span><br></pre></td></tr></table></figure><h4 id="定义神经网络模型"><a href="#定义神经网络模型" class="headerlink" title="定义神经网络模型"></a>定义神经网络模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 2.Define a Convolutional Neural Network</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(<span class="string">"Model Created."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#%% Use GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">  <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">  net = nn.DataParallel(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.to(device)  <span class="comment"># to GPU</span></span><br><span class="line">print(<span class="string">"Model to GPU."</span>)</span><br></pre></td></tr></table></figure><h4 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 3. Define a Loss function and optimizer</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># to GPU</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">print(<span class="string">"Loss and Optimizer Function Defined."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># to GPU</span></span><br><span class="line">criterion.to(device)</span><br><span class="line">print(<span class="string">"Loss Function To GPU."</span>)</span><br></pre></td></tr></table></figure><h4 id="训练神经网络模型"><a href="#训练神经网络模型" class="headerlink" title="训练神经网络模型"></a>训练神经网络模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 4. Train the network</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># to GPU</span></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save our trained model</span></span><br><span class="line">PATH = <span class="string">'./cifar_net.pth'</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure><h4 id="验证模型"><a href="#验证模型" class="headerlink" title="验证模型"></a>验证模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 5. Test the network on the test data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load back in our saved model</span></span><br><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">outputs = net(images)</span><br><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><h4 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h4><p>运行程序，输出：<br><img src="http://q8b0crcx6.bkt.clouddn.com/gpuresult.png" width="60%" height="60%" alt="输出" align="center"><br>查看GPU状态，确认使用了GPU：<br><img src="http://q8b0crcx6.bkt.clouddn.com/running.png" width="60%" height="60%" alt="训练中" align="center"></p><center><font size="1">训练中</font> </center><br><img src="http://q8b0crcx6.bkt.clouddn.com/pending.png" width="60%" height="60%" alt="训练结束" align="center"><br><center><font size="1">训练结束</font> </center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用NVIDIA GPU进行运算，与使用CPU计算相比，算法代码需要进行改动的有&lt;br&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://smartchar.cn/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch | 线性回归模型训练</title>
    <link href="https://smartchar.cn/2020/04/18/PyTorch%20-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    <id>https://smartchar.cn/2020/04/18/PyTorch - 线性回归模型训练/</id>
    <published>2020-04-18T12:01:44.000Z</published>
    <updated>2020-04-18T12:25:22.368Z</updated>
    
    <content type="html"><![CDATA[<p>示例代码：<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Training Dataset</span></span><br><span class="line">data_x = torch.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">data_y = torch.linspace(<span class="number">2</span>,<span class="number">11</span>,<span class="number">10</span>)</span><br><span class="line">plt.scatter(data_x.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),data_y.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),label = <span class="string">'data'</span>,color = <span class="string">'red'</span>)</span><br><span class="line">myDataSet = Data.TensorDataset(data_x,data_y)</span><br><span class="line">myDataLoader = Data.DataLoader(dataset = myDataSet,batch_size = <span class="number">1</span>,shuffle = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Linaer Regression Model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Linear,self).__init__()</span><br><span class="line">        self.f1 = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.f1(x)        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Model</span></span><br><span class="line">net = Linear()</span><br><span class="line">criterion = nn.L1Loss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(),lr = <span class="number">0.01</span>, momentum = <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train Model</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">60</span>):</span><br><span class="line">    <span class="keyword">for</span> i,data <span class="keyword">in</span> enumerate(myDataLoader,<span class="number">0</span>):</span><br><span class="line">        x,y_target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        y_hat = net(x)</span><br><span class="line">        loss = criterion(y_hat,y_target)</span><br><span class="line">        loss.backward()    </span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">19</span> == <span class="number">0</span>:</span><br><span class="line">        y_hat = net.state_dict()[<span class="string">'f1.weight'</span>] * data_x + net.state_dict()[<span class="string">'f1.bias'</span>]</span><br><span class="line">        plt.plot(data_x.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),y_hat.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),label = <span class="string">'prediction'</span>)</span><br><span class="line">        <span class="comment"># plt.legend()</span></span><br></pre></td></tr></table></figure></p><p>数据拟合结果：<br><img src="http://q8b0crcx6.bkt.clouddn.com/linear_regression.png" width="50%" height="50%" alt="线性回归" align="center"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;示例代码：&lt;br&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://smartchar.cn/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>朴实无华枯燥生活 - Robotics课程学习进展喜人</title>
    <link href="https://smartchar.cn/2020/04/10/%E6%9C%B4%E5%AE%9E%E6%97%A0%E5%8D%8E%E6%9E%AF%E7%87%A5%E7%94%9F%E6%B4%BB%20-%20Robotics%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E8%BF%9B%E5%B1%95%E5%96%9C%E4%BA%BA/"/>
    <id>https://smartchar.cn/2020/04/10/朴实无华枯燥生活 - Robotics课程学习进展喜人/</id>
    <published>2020-04-10T13:47:44.000Z</published>
    <updated>2020-04-23T09:29:36.985Z</updated>
    
    <content type="html"><![CDATA[<h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul><li>桌面动态画面截图<br>  ScreenToGif，Github开源项目，功能齐全，操作顺手，帮助我录制下了我这几天的Coursera课程成果。  <a id="more"></a></li><li><p>图床<br>  七牛云的口碑比较不错，可以当做静态图和动态图的图床。缺点是想要使用免费的图床空间，我需要上传自己的身份证进行实名认证。没办法，为了方便，我还是牺牲了自己的隐私。</p><p>  <img src="http://q8b0crcx6.bkt.clouddn.com/e1.gif" width="50%" height="50%" alt="Dolly Zoom Effect" align="center"></p>  <center><font size="1">Dolly Zoom Effect</font> </center><br>  <img src="http://q8b0crcx6.bkt.clouddn.com/e2.gif" width="50%" height="50%" alt="Image Projection" align="center"><br>  <center><font size="1">Image Projection</font> </center><br>  <img src="http://q8b0crcx6.bkt.clouddn.com/e3.gif" width="50%" height="50%" alt="AR" align="center"><br>  <center><font size="1">AR</font> </center><br>  <img src="http://q8b0crcx6.bkt.clouddn.com/e4.gif" width="50%" height="50%" alt="Structure from Motion" align="center"><br>  <center><font size="1">Structure from Motion</font> </center></li></ul><h4 id="楼下"><a href="#楼下" class="headerlink" title="楼下"></a>楼下</h4><p><img src="http://q8b0crcx6.bkt.clouddn.com/shequlouxia.png" width="50%" height="50%" alt="劫后余生" align="center"></p><center><font size="1">劫后余生</font> </center><h4 id="看剧"><a href="#看剧" class="headerlink" title="看剧"></a>看剧</h4><p>温暖的家庭总能治愈人，而关系疏远、相处模式奇怪的家庭让人需要被治愈。</p><p>小欢喜情节可能不太真实但是演员们的表演细节非常真实。不真实之处有区长家庭环境和条件不符合普通人（比如本人）对权势家庭的认知，区长更像是编剧特意塑造的红色经典模范。季杨杨开车进入学校引起轰动的场景，有点夸张了。但是不得不说演员的表演非常到位的，比如黄磊，哎我只记得我看剧的时候有几处感叹他的表演真是到位，现在竟然一点都想不起来是哪里了。海清每一集都要掉很多眼泪，但是也没有什么违和感。沙溢有一个地方我觉得比较真实、印象比较深刻：在「学生家长畅谈会」上，老乔、宋倩和英子坐在三个背靠背的椅子上，还没有开始谈话时老师推门进来了，宋倩抬头跟老师说话时，老乔听到声音才回头看发生什么事了，又立刻转回去了。这一点很符合人的正常反应：有人进来了&gt;因为好奇看一眼发生什么了&gt;发现没我啥事就继续该干啥干啥了。</p><p>小欢喜是由无数善意的、恶意的谎言构成的剧，依靠「撒谎&gt;揭穿谎言」这种模式推动情节发展。方一凡谎称为了学习想搬家，因此从英子那里骗的安眠药、给磊儿服药；英子瞒着母亲把乐高玩具藏在方一凡家里、老乔家里；老乔偷偷搬到书香雅苑给英子提供「避难所」；刘静谎称自己出差其实时住院；方圆瞒着童文杰给方一凡安排艺考…大冲突推到情节发展，小冲突刻画人物细节，最终将每个人的形象都刻画的圆圆满满。</p><p><img src="http://q8b0crcx6.bkt.clouddn.com/xiaohuanxi.jpg" width="100%" height="100%" alt="乔英子" align="center"></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;工具&quot;&gt;&lt;a href=&quot;#工具&quot; class=&quot;headerlink&quot; title=&quot;工具&quot;&gt;&lt;/a&gt;工具&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;桌面动态画面截图&lt;br&gt;  ScreenToGif，Github开源项目，功能齐全，操作顺手，帮助我录制下了我这几天的Coursera课程成果。
    
    </summary>
    
    
      <category term="Live" scheme="https://smartchar.cn/tags/Live/"/>
    
  </entry>
  
  <entry>
    <title>雅思考试回顾总结</title>
    <link href="https://smartchar.cn/2019/11/01/%E9%9B%85%E6%80%9D%E8%80%83%E8%AF%95%E5%9B%9E%E9%A1%BE%E6%80%BB%E7%BB%93/"/>
    <id>https://smartchar.cn/2019/11/01/雅思考试回顾总结/</id>
    <published>2019-11-01T08:18:20.000Z</published>
    <updated>2020-04-23T08:18:18.556Z</updated>
    
    <content type="html"><![CDATA[<p>英语基础：CET4-591，CET6-520<br>    <a id="more"></a><br>第一次考雅思2019年4月6日，纸笔（南京），复习15天，成绩：Overall 6.5 (L7.5，R6.5，W5.5，S6.0)</p><p>第二次考雅思2019年9月10日，机试（上海），复习30天，成绩：Overall 7.0 (L7.5，R7.0，W6.5，S6.5)</p><h4 id="听"><a href="#听" class="headerlink" title="听"></a>听</h4><p>听力与口语息息相关，心中熟悉音标、嘴上发音标准，这样才能在做听力时快速地将听到的单词发音与脑子里的单词意义对应，考口语时让考官没有障碍地理解你说的每个单词。我是通过看YouTube频道上的《谢丽媛发音篇》》来逐个学习音标、纠正自己以前未注意到的发音误区。</p><p>针对雅思的听力我没有进行特别的训练，主要依赖于日常听英文播客或者视频栏目，约每天30分钟。一方面是为了备考雅思，另一方面也是因为自己本身对栏目的兴趣。栏目如下</p><p>播客栏目</p><p>99% Invisible：人文类，讲述世界多种地区发展的故事。主题区分明显，因此生词可能比较多。但是官网上有对应的原稿，听完一遍之后看一遍原稿，重点听之前没有听清楚的地方。或者第一次听之前，像预习课文一下读一遍原稿，心里大致知道会讲哪些东西。熟练之后就可以逐渐不用看原文也可以听懂了。</p><p>Intelligence Square Debate U.S.：辩论节目，议题主要涉及政治政策。官网有原稿，语速正常，吐词清晰。适合对辩论或者时事感兴趣的学习者。</p><p>YouTube频道</p><p>TED：个人演讲，包罗万象——教育、设计、科学研究、人文探索，常会感叹「原来有人在研究这个」。刚开始可能难以跟上时可以在不懂的地方打开字幕，但是提升听力能力建议一定要关掉字幕观看。</p><p>Vox：科普/新闻，讲历史、科技、发展等。</p><p>Vice News：新闻频道，每天报道世界各地事件。</p><p>The Verge：科技媒体，手机、电脑等电子设备测评。</p><p>Great Big Story：制作精美的小故事，讲述不同地区的风俗人文。</p><p>storybooth：读者投稿类故事节目，节目组添加动画形成视频。由于是读者自己朗读自己的故事，所以难免会有一定口音。但是我觉得这个里面的词语和句式很口语化，适合日常对话中使用，因此我是跟读每个视频尝试熟练背一两句句式，丰富自己的口语语料。</p><p>针对雅思考试，我推荐的资料是《雅思王听力真题语料库》，听这个可以很好的训练发音与单词的快速对应能力，还可以检查自己单词的发音是否准确。临近考试之前最好的训练方法是做剑桥试题。</p><h4 id="说"><a href="#说" class="headerlink" title="说"></a>说</h4><p>说分为两个部分：说的准确和有话说。说的准确是指发音标准、语调自然连贯，这一部分通过模仿与纠正来实现；有话说是指针对某一主题，脑子里有足够多的相关的单词、句式来支撑你表达自己想法。比如设想在学术会议上交流的场景，当讲者做完报告之后，我需要在脑子里整理自己的问题、搜索这个学科的相关词汇、提问句式，最后将问题清晰地表达出来。</p><p>在学习发音之前，我觉得应该先区分英音和美音。虽然九年义务教育和高中大学英语教育没有重视过区分英音和美音的不同，但是它们的差异不小。比如「mask」，英音读/mɑːsk/，美音读/mæsk/，混用英音和美音对做听力影响很大。通过前面提到的《谢丽媛发音篇》，我觉得可以解决绝大部分发音不标准的问题。此外碰到不熟悉或者发音没有把握的单词就查字典，逐渐将音标与单词一一对应。</p><p>另一方面是语调问题，正常的英语交流是抑扬顿挫的。前面提到的播客栏目和YouTube频道，选取自己感兴趣的部分模仿与跟读。拿手机录下自己的声音，然后回放。一遍遍纠正之后，你会逐渐形成新的语调。进行这个步骤的学习时，不妨「矫枉过正」，故意夸张地放大一句话里面的升调和降调。因为其实真正与人实景交流时，由于害羞、或者担心自己的语调不对，人难免不会像自己练习时那样升降明显。</p><p>针对「有话说」这个方向的积累，我觉得比较好的单人训练的方法是给自己随机定一个主题，然后计时五分钟，强迫自己说满这五分钟。事后听录音时你会发现自己词汇量和句式的贫乏，可能是重复说一些东西。然后就上网搜索这个主题——用英文搜索，把自己设想成母语为英语，积累单词和表达。每天一个主题，一年的积累量将是可观的。</p><p>此外为了使学习过程有趣一点，可以关注学校里面的语言相关的活动。比如国际教育学院的一些活动需要招募语言类志愿者，我参加过一次Summer School，全程与外国学生交流，验证自己阶段学习成果，提升自己的自信心。</p><p>请外教也是个短期提升对话能力的一个好办法。目前（2019年8月）市场价格约1.5~2.0元/分钟。我在考雅思之前使用伴鱼app，上面有来自各个国家的职业或业余英语老师提供一对一口语辅导。然而，我觉得请外教主要用于提升自己的音标、语调和语感，并不能短期内丰富你自己的口语语料库——即你不知道说什么最终还是不知道说什么。所以建议在词汇量和句式库积累到一定程度后，再请外教来提升。</p><p>针对雅思考试，我推荐的资料是《慎小嶷——十天突破雅思口语》，适合目标为6.5分的人群。</p><h4 id="读"><a href="#读" class="headerlink" title="读"></a>读</h4><p>读对于大多数人来说应该不在话下，掌握足够的词汇量和扎实的语法基础即可：背单词是为了认识它，学语法是为了看懂句子。所以这一步的重点是背单词和学语法。</p><p>背单词我推荐使用欧陆辞典，它有两个优点：一、可以导入<a href="https://pan.baidu.com/s/1C8kYr0Rbfxb7OYb--KmrDA" target="_blank" rel="noopener">辞典库</a>，比如牛津高阶辞典和搭配辞典；二、背单词的卡片上显示的即是专业辞典的释义，而不是想百词斩一样的图片，这样最终记住的是释义而不是图片。而且具备移动端和桌面端之间的同步功能。</p><p>学语法我暂时没有好的学习资料，依靠自己高中的英语积累。</p><p>掌握了词汇量和语法之后，需要日常不断的练习，比如阅读英语报刊、书籍、文献等。这里我推荐的报刊有：</p><p>The Economist：英国媒体，周刊，涉及政治、文化、科技。逻辑完整、用词考究，既适合用来练习阅读，也适合用来模仿写作。</p><p>SCMP：South China Morning Post，香港媒体，日报，这个用来了解新闻、保持日常阅读量即可，不建议模仿新闻报道写作。</p><p>针对雅思考试，我没有进行单独的训练，只在考试之前熟悉了模拟试题上各个部分的用时分配。</p><h4 id="写"><a href="#写" class="headerlink" title="写"></a>写</h4><p>写是我最不擅长的一个方面，因为日常几乎没有什么工作是涉及到英语写作的。我第一次考雅思时信心满满，以为跟高考英语作文差不多，结果滑铁卢。第二次认认真真地啃《慎小嶷——十天突破IELTS写作完整真题库与6-9分范文全解》，背诵范文结构与句式，模拟写作约四十篇。第二次发挥就好多了。</p><p>资源下载</p><hr><ol><li><a href="https://pan.baidu.com/s/1C8kYr0Rbfxb7OYb--KmrDA#list/path=%2F" target="_blank" rel="noopener">欧陆辞典词库</a></li><li><a href="https://pan.baidu.com/s/1aukZ9C57ffAPjQ2IeywdxA#list/path=%2F" target="_blank" rel="noopener">《谢丽媛发音篇》</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;英语基础：CET4-591，CET6-520&lt;br&gt;
    
    </summary>
    
    
      <category term="英语" scheme="https://smartchar.cn/tags/%E8%8B%B1%E8%AF%AD/"/>
    
      <category term="考试" scheme="https://smartchar.cn/tags/%E8%80%83%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>2018年北京驱逐「低端人口」事件</title>
    <link href="https://smartchar.cn/2018/09/16/2018%E5%B9%B4%E5%8C%97%E4%BA%AC%E9%A9%B1%E9%80%90%E3%80%8C%E4%BD%8E%E7%AB%AF%E4%BA%BA%E5%8F%A3%E3%80%8D%E4%BA%8B%E4%BB%B6/"/>
    <id>https://smartchar.cn/2018/09/16/2018年北京驱逐「低端人口」事件/</id>
    <published>2018-09-16T07:29:06.000Z</published>
    <updated>2020-04-11T05:13:30.383Z</updated>
    
    <content type="html"><![CDATA[<p>2017年11月18号，北京市大兴区的一栋群租住房发生重大火灾事故，造成19人死亡，8人受伤。<br><a id="more"></a><br>大火导致北京市内燃起为期 40 天的「安全隐患大排查、大清理、大整治专项行动」，聚居在市内出租公寓、工业园区等地的大量外来人口被迫搬迁。与此次大火一同引发民众关注的，还有「低端人口」一词。虽然官方急于与该词撇清关系，网民们仍利用网络搜索引擎查到了北京市多个地区政府公文中出现「低端人口」。</p><p>我未想到政府会如此毫不留情地在冬日的寒风中驱赶他的公民，更难以想象政府会如此坦荡地用「低端人口」来称呼自己的同胞。</p><h4 id="溯源"><a href="#溯源" class="headerlink" title="溯源"></a>溯源</h4><p>2002年，申奥成功的北京因为奥运设施兴建以及城市风貌整改等计划，大批非京籍人口开始涌入。北京市人口不断增加，城市空气污染问题、交通堵塞问题越来越严重，北京市开始制定多种控制人口相关政策：</p><p>2005年《北京城市总体规划（2004-2020）》指出，2020年北京市总人口规模计划控制在1800万左右。</p><p>2006年《北京市“十一五”时期重点新城发展实施规划》中，其中「完善人口发展相关政策」一节提出：通过产业结构调整、减少低端产业、强化外来人口管理、加强出租房屋管理、拆除违法建设和改造「城中村」，以及加强本地人口尤其是农业人口的就业培训、强化城市管理等综合调控与管理手段，防止低端外来人口在新城大规模聚集，进一步加大高端人才的引进力度，提高新城人口素质和优化人口结构。这也是「低端人口」这一词较早的出现源头。</p><p>2010年，北京市政府工作报告提出“通过城市功能疏解、产业结构升级和布局调整，促进人口有序迁移与合理分布。</p><p>2011年1月，在北京市工商行政管理工作会议上，北京市政府表示要「屏蔽不符合首都功能定位的行业形态」，提出了「以业控人」的「加减法」规划，小百货等17类业态将提升审批准入门槛，约涉及30万户商业主体、100万流动人口。同年8月，北京市政府公布《北京市人民防空工程和普通地下室安全使用管理办法》，禁止出租地下室。</p><p>2014年，北京市政府首次提出人口增速明显下降的目标，并提高非京籍适龄儿童入读公办小学门槛。</p><p>2015年，北京市政府提出疏解北京非首都功能，包括构建「高精尖经济结构」和空间结构等。</p><p>2016年，北京提出的常住人口控制目标是2300万；下半年，北京居住证制度计划正式实施，确定了严格的落户门槛。</p><p>2017年年初，北京开展「疏解整治促提升」专项行动，主要包括拆除违法建设、整治「开墙打洞」、城乡结合部整治改造等十个方向，并设定目标值。</p><p>北京不欢迎你。</p><p>2017年11月18日，北京市大兴区的一栋群租住房发生重大火灾事故，造成19人死亡，8人受伤。旋即政府以「安全清查」为名，采取断水断电、断供暖、断燃气、限期搬家、强行清理、行政拘留等手段，驱逐了北京市13个区的约292处居住点，涉及群众近十万人。无辜的租客未得到任何房租补偿以及后期安置，在北京的寒风中匆匆寻找下一处落脚点。</p><h4 id="发问"><a href="#发问" class="headerlink" title="发问"></a>发问</h4><p>人权、法治、言论——在此次「2017年11月北京驱逐「低端人口」」事件中被踩在地上蹂躏。</p><p>人之所以比动物高级，是因为我们具有同情之心。而此次事件中，北京政府对待自己的同胞却不见有丝毫的同情心，执法人员视上级行政命令大于天，在寒冷的冬日粗暴地驱逐无辜民众至落难街头。</p><p>此次驱逐行动程序合法吗？本次驱逐行动缺乏公众参与、缺乏专家论证、缺乏风险评估、缺乏合法性审查、缺乏集体讨论，如此草率的行动的程序正义何在？另一方面，虽然清理“违章建筑”有法理依据，但处理的对象应是房主而非租客。现在北京市却让租客去承受所有恶果，更不用提「补偿」租客财产与精神损失的问题。此二者严重违背  「依法治国」的理念。</p><p>事发之后，北京政府一如既往地控制网上言论。社交媒体上与驱逐事件有关的博文或资讯遭到封杀，百名知识分子联名上书的新闻遭到禁言，民间团体自发援助受灾民众却因为「众所周知的原因」被迫中止。何以使政府对舆论如此担惊受怕？我不得而知。</p><p>一场驱逐事件，将我国长期存在的矛盾暴露无遗。</p><p>在资源分配不均的情况下，人自然会向资源丰富的地区聚集，如同动物为了适应环境而迁徙一般，人也追求更好的生活。中国从1968年开始实行的户籍制度和人事档案制度，户籍制度将公民划分成「城市户口」和「农村户口」，旧时依据不同户口类别分配生活物资，而直到当今，不同地区或不同类别的户口在教育、医疗、社会福利等方面的仍差别悬殊；人事档案制度要求用人单位保留员工个人档案，员工入职新单位时须经原单位同意后迁出、经新单位同意后迁入个人档案。</p><p>户籍制度和人事档案制度如同两座大山，横在每一个渴望自由迁徙的人前。若要使各地区都发展均衡实属难题，那么以户籍制度和人事档案制度简单地限制人口的流动，则是执政者思想上的懒惰和既得利益者自私自利的本质体现。</p><p>除此之外， 立法、司法制度的落后亦在此次事件中尽显。立法过程是否合法？涉及基层民生的法律和政策却不是由基层民众参与制定或是讨论；执法过程是否合法？根据网上的视频，手持警棍的执法人员踹开出租屋、收缴私人物品，官方如此的行为着实令人不安。数万甚至数十万「低端人口」在数天之内被通知「安全大清查」，被强行要求离开住处。他们事先不知情，事情发生时无处发声，事后申诉、索赔无门。因为一纸通知就是大于天的王法，「低端人口」丝毫没有说「不」的权利。</p><p>2003年，大学生孙志刚在收容所内身亡，当时事件尚因有南方都市报的记者深入跟踪报道而引起全国广泛关注，并直接推动废除实行多年的“收容制度”。而在此次驱逐事件中，已经难以看到有媒体跟进，更不用说进行深入调查反思事件的起因和制度缺漏，进而推动制度改革。自由媒体之死，亦即千万百姓之哀。</p><h4 id="尊严"><a href="#尊严" class="headerlink" title="尊严"></a>尊严</h4><p>我家附近可以碰到形形色色的人，清理街道的环卫工，早点摊位上卖热干面的大妈，夏天卖啤酒的冷饮店老板……按照北京政府的定义，社会底层的朴实众生有几个不是「低端人口」？工作环境恶劣、工作时间长、职业不被人们尊重等因素是客观环境，但主观上他们和千百万同胞一样，无论「低端」与「高端」，都是憧憬生活，充满希望；辛勤耕耘，朝夕奔梦。</p><p>人们常常以人口对经济的贡献来衡量人的价值，因此各地落户政策设置门槛，高素质人才可以撑起本地的经济繁荣，因此他们被接纳。貌似合情合理的逻辑即是问题的根源：从何时起，人被作为一种资本，而一个人的价值也被等同于他或她所能生产出的经济效益？</p><p>社会各式各样的制度赋予了人不同的身份和标识，以此提供了人群互相歧视的场域。但愿每个人都不迷失在既得利益的熏陶中，仍视人为鲜活的个体，而不是表格上代表劳动力与生产力的一个个数据点。我们有所思、有所求，我们都需要独立而有尊严的活着。</p><p>引用标注：<br>本文引用的数据及相关政策来自端传媒、维基百科。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2017年11月18号，北京市大兴区的一栋群租住房发生重大火灾事故，造成19人死亡，8人受伤。&lt;br&gt;
    
    </summary>
    
    
      <category term="社会" scheme="https://smartchar.cn/tags/%E7%A4%BE%E4%BC%9A/"/>
    
  </entry>
  
</feed>
