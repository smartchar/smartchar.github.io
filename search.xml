<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NUA2 Thesis | 公式、图表、单位的处理 （3）</title>
      <link href="/2020/05/07/NUA2%20Thesis%20-%20%E5%85%AC%E5%BC%8F%E3%80%81%E5%9B%BE%E8%A1%A8%E3%80%81%E5%8D%95%E4%BD%8D%E7%9A%84%E5%A4%84%E7%90%86%20%EF%BC%883%EF%BC%89/"/>
      <url>/2020/05/07/NUA2%20Thesis%20-%20%E5%85%AC%E5%BC%8F%E3%80%81%E5%9B%BE%E8%A1%A8%E3%80%81%E5%8D%95%E4%BD%8D%E7%9A%84%E5%A4%84%E7%90%86%20%EF%BC%883%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>&emsp;<br><a id="more"></a></p><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>如果你之前一直在用MathType输入公式，那么刚开始使用LaTeX语法输入公式可能有点陌生，但是没关系，你马上就是适应这种输入公式的感觉。LaTeX语法中公式也分为行内公式和行间公式：行内公式用成对的<code>$</code>符号将字母公式括起来，像这样<code>$</code>y=2x<code>$</code>；行间公式像这样使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">y = 2x</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p><p>有时候我们在正文中某个地方需要引用某个公式，那么我们需要先给公式一个唯一的身份，使用<code>\label</code>命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">y = 2x</span><br><span class="line">\label&#123;eq:dynamics&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p><p><code>\label{}</code>中的文本可以任意输入，只要不与其它公式重复即可。然后在正文中，使用<code>\ref{eq:dynamics}</code>就可以引用这个公式了。你无需关心这个公式是1-2或者是3-5，LaTeX模板已经替你考虑好了，它会自动按照公式所在的章节进行编号。</p><p>一开始你可能不好应付非常复杂的公式，毕竟LaTeX语法不是所见即所得的。你可以借助MathType来生成LaTeX格式的公式：MathType中打开<code>Preference</code>&gt;<code>Cut and Copy Preferences</code>，设置成<code>MathML or TeX</code> &gt; <code>Plain TeX</code>，此时可以先在MathType输入好公式，然后复制这个公式，粘贴就可以得到LaTeX格式的公式了。注意，对于MathType输入的行间公式，复制粘贴之后会得到<code>\[y=2x\]</code>这种格式，我们将它放到<code>\begin{equation}</code>和<code>\end{equation}</code>之间的时候，只需要复制<code>\[</code>和<code>\]</code>之间的内容，否则公式会报错。</p><h2 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h2><p>图的使用格式是这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[h!]</span><br><span class="line">\includegraphics[scale=1.0]&#123;2-4-control.png&#125;</span><br><span class="line">\caption&#123;磁悬浮轴承系统控制框图&#125;</span><br><span class="line">\label&#123;fig:2-4-control&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure></p><h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2>]]></content>
      
      
      
        <tags>
            
            <tag> NUA2 Thesis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NUA2 Thesis | 内容填充 (2)</title>
      <link href="/2020/05/06/NUA2%20Thesis%20-%20%E5%86%85%E5%AE%B9%E5%A1%AB%E5%85%85%20%EF%BC%882%EF%BC%89/"/>
      <url>/2020/05/06/NUA2%20Thesis%20-%20%E5%86%85%E5%AE%B9%E5%A1%AB%E5%85%85%20%EF%BC%882%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>&emsp;<br><a id="more"></a></p><h2 id="1主文件-master-tex-结构"><a href="#1主文件-master-tex-结构" class="headerlink" title="1主文件 master.tex 结构"></a>1主文件 <code>master.tex</code> 结构</h2><ul><li><p>格式定义</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[</span><br><span class="line">lang=cn,  </span><br><span class="line">degree=master,</span><br><span class="line">% zhuanshuo,</span><br><span class="line">openany,oneside</span><br><span class="line">% openright,blankleft,twoside</span><br><span class="line">]&#123;nuaathesis&#125;</span><br></pre></td></tr></table></figure><p>  这一部分定义了文档的全局格式，lang=cn指使用中文，degree=master指使用研究生论文模板，%是LaTeX语言的注释符号，% zhuanshuo 指使用学硕模板（如果要使用专硕模板就将%去掉），写论文时使用openany,oneside，需要提交图书馆打印时，将openany,oneside注释掉，将openright,blankleft,twoside取消注释，从而打印摘要、承诺书右侧空白的界面。</p></li><li><p>文档标记</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;document&#125;</span><br><span class="line">...</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>  这是文档的开始和结束标记。</p></li><li><p>封面、承诺书等标准化界面</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\makecover</span><br><span class="line">\makedeclare</span><br><span class="line">\frontmatter</span><br><span class="line">\makeabstract</span><br><span class="line">% 如果需要调整目录层级数量的话，取消下一行注释，数字含义: 0=chapter, 1=section, 2=subsection</span><br><span class="line">% \setcounter&#123;tocdepth&#125;&#123;1&#125;</span><br><span class="line">\nuaatableofcontents</span><br><span class="line">\nuaalistoffigurestables</span><br></pre></td></tr></table></figure><p>  根据英文名称就可以看出来这几行代码分别是输出封面、承诺书、摘要、目录和图表列。</p></li><li><p>内容填充 </p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">\include&#123;content/abbr&#125;</span><br><span class="line"></span><br><span class="line">\mainmatter</span><br><span class="line"></span><br><span class="line">\include&#123;content/start&#125;</span><br><span class="line">\include&#123;content/highlight&#125;</span><br><span class="line">\include&#123;content/theorem2&#125;</span><br><span class="line">\include&#123;content/demo&#125;</span><br><span class="line"></span><br><span class="line">\appendix</span><br><span class="line">% 如果需要附录的话，在这里 include</span><br><span class="line">\include&#123;content/ex_postscript&#125;</span><br><span class="line"></span><br><span class="line">\backmatter</span><br><span class="line">% 如果参考文献使用 biber</span><br><span class="line">\bibliographystyle&#123;nuaabib&#125;   % 参考文献的样式</span><br><span class="line">\bibliography&#123;bib/sample&#125;   % 参考文献，即 bib/sample.bib 文件（纯文本）</span><br><span class="line">% 如果打算手写参考文献</span><br><span class="line">\include&#123;content/manref&#125;</span><br><span class="line"></span><br><span class="line">\include&#123;content/acknowledge&#125;</span><br><span class="line">\include&#123;content/backmatter&#125;</span><br></pre></td></tr></table></figure><p>  include是用来插入存放在其他文件中的内容，类似C语言的include用法。这个模板把内容分开放在了不同的.tex源文件中，然后通过include聚合在一起。</p></li></ul><h2 id="2-填充内容"><a href="#2-填充内容" class="headerlink" title="2 填充内容"></a>2 填充内容</h2><h3 id="第一章到第六章"><a href="#第一章到第六章" class="headerlink" title="第一章到第六章"></a>第一章到第六章</h3><p>NUA2 Thesis 模板的思想是将不同章节放在不同的<code>.tex</code>文件中保存，然后通过<code>include</code>命令将所有章节的<code>.tex</code>文件聚合在一起。我不打算用这种结构，原因是：Texmaker编辑器可以识别一个<code>.tex</code>文件内的结构，并以大纲的形式显示在左侧。如果使用多个文件存放不同章节的内容，那么就不能显示大纲了，只会显示文件列表。</p><p>我们先来删掉<code>master.tex</code>中的以下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\include&#123;content/start&#125;</span><br><span class="line">\include&#123;content/highlight&#125;</span><br><span class="line">\include&#123;content/theorem2&#125;</span><br><span class="line">\include&#123;content/demo&#125;</span><br><span class="line"></span><br><span class="line">\appendix</span><br><span class="line">% 如果需要附录的话，在这里 include</span><br><span class="line">\include&#123;content/ex_postscript&#125;</span><br></pre></td></tr></table></figure></p><p>然后在<code>\mainmatter</code>和<code>\backmatter</code>两行之间像这样填充论文第一章至最后一章的内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">\mainmatter</span><br><span class="line"></span><br><span class="line">\chapter&#123;绪论&#125;</span><br><span class="line">本章介绍磁悬浮轴承...</span><br><span class="line"></span><br><span class="line">\section&#123;磁悬浮轴承研究现状&#125;</span><br><span class="line">众所周知...</span><br><span class="line"></span><br><span class="line">\subsection&#123;国内&#125;</span><br><span class="line">众所周知...</span><br><span class="line"></span><br><span class="line">\subsection&#123;国外&#125;</span><br><span class="line"></span><br><span class="line">\chapter&#123;磁悬浮轴承原理&#125;</span><br><span class="line">\section&#123;磁悬浮轴承动力学模型&#125;</span><br><span class="line">\subsection&#123;机械结构&#125;</span><br><span class="line">\subsection&#123;控制原理&#125;</span><br><span class="line"></span><br><span class="line">\backmatter</span><br></pre></td></tr></table></figure></p><p>按<code>Ctrl</code>+<code>s</code>保存之后，点击<code>Run</code>等待编译完，然后点<code>View</code>，就可以看到生成的文档啦：<br><img src="L2-1.jpg" alt=""></p><p>除了第一章至最后一张的内容需要填充，我们还需要修改封面、摘要、致谢等信息。</p><h3 id="封面（中文和英文）"><a href="#封面（中文和英文）" class="headerlink" title="封面（中文和英文）"></a>封面（中文和英文）</h3><p>在<code>global.tex</code>文件中修改</p><h3 id="承诺书"><a href="#承诺书" class="headerlink" title="承诺书"></a>承诺书</h3><p>无需修改，自动生成</p><h3 id="摘要（中文和英文）"><a href="#摘要（中文和英文）" class="headerlink" title="摘要（中文和英文）"></a>摘要（中文和英文）</h3><p>在<code>global.tex</code>文件中修改</p><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p>无需修改，自动生成</p><h3 id="图标清单"><a href="#图标清单" class="headerlink" title="图标清单"></a>图标清单</h3><p>无需修改，自动生成</p><h3 id="注释表"><a href="#注释表" class="headerlink" title="注释表"></a>注释表</h3><p>在<code>content\abbr.tex</code>中修改</p><h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>在<code>content\acknowledge.tex</code>中修改</p><h3 id="在学期间的研究成果及学术论文情况"><a href="#在学期间的研究成果及学术论文情况" class="headerlink" title="在学期间的研究成果及学术论文情况"></a>在学期间的研究成果及学术论文情况</h3><p>在<code>content\backmatter.tex</code>中修改</p><h3 id="填充参考文献"><a href="#填充参考文献" class="headerlink" title="填充参考文献"></a>填充参考文献</h3><p>我们先将所有需要用到的参考文献以条目的形式储存在<code>bib\sample.bib</code>中，然后在需要正文需要引用的地方「艾特」某条参考文献即可，使用流程如下：</p><p>我需要用到的参考文献的是《The internal model principle for linear multivariable regulators》，在谷歌学术或者百度学术中搜索这篇文章，下载得到这种格式的条目：</p><p><img src="L2-2.jpg" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@article&#123;francis1975internal,</span><br><span class="line">title=&#123;The internal model principle for linear multivariable regulators&#125;,</span><br><span class="line">author=&#123;Francis, Bruce A and Wonham, William M&#125;,</span><br><span class="line">journal=&#123;Applied mathematics and optimization&#125;,</span><br><span class="line">volume=&#123;2&#125;,</span><br><span class="line">number=&#123;2&#125;,</span><br><span class="line">pages=&#123;170--194&#125;,</span><br><span class="line">year=&#123;1975&#125;,</span><br><span class="line">publisher=&#123;Springer&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>把这一条放置在<code>bib\sample.bib</code>的任意位置就相当于把这条参考文献添加到了你自己的引用库中了，下面就可以在正文的任意位置引用它了。像这样引用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是一条引用\cite&#123;francis1975internal&#125;。</span><br></pre></td></tr></table></figure><p>添加完引用后，我们编译过程需要额外进行一次<code>BibTex</code>编译<code>master.tex</code>，才能让参考文献正确地显示在正文中。完整的编译流程是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">XeLaTeX &gt; BibTeX &gt; XeLaTeX &gt; XeLaTeX &gt; View pdf</span><br></pre></td></tr></table></figure><p><code>BibTex</code>听起来有点陌生，你不用关心这是什么，只用知道<code>BibTex</code>和<code>XeLaTex</code>在同一个列表中，选择之后点击左侧的三角形<code>Run</code>图标即开始编译了。</p><p><img src="L2-3.jpg" alt=""></p><p>可能你也看出来了，一共需要点击5次<code>Run</code>才能看到最终的pdf文档，这不符合这个模板「解放劳动力」的宗旨，好在Texmaker编辑器给我们提供了<code>Quick Build</code>功能。点击上方菜单栏的<code>Option</code>&gt;<code>Configure Texmaker</code>，弹出配置界面后按照下图序号所示的步骤进行配置，点<code>OK</code>完成配置。现在，我们就可以<code>Run</code>这个<code>Quick Build</code>，稍等一会儿就可以看到新生成的文档了：</p><p><img src="L2-4.jpg" alt=""></p><p><img src="L2-5.jpg" alt=""></p><p>至此，你已经掌握了这个模板的90%的内容，余下一点简单的公式、图标和单位的处理。</p>]]></content>
      
      
      
        <tags>
            
            <tag> NUA2 Thesis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NUA2 Thesis | Hello World ! (1)</title>
      <link href="/2020/05/04/NUA2%20Thesis%20-%20Hello%20World%20%EF%BC%881%EF%BC%89/"/>
      <url>/2020/05/04/NUA2%20Thesis%20-%20Hello%20World%20%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>&emsp;</p><p>使用Microsoft Word来写毕业论文操作简单，所见即所得。但是在文档涉及大量图片、公式时，文档启动、翻页、保存过程就不是很流畅。此外，学位论文的格式要求繁琐，比如字体、段落间隔、缩进、公式编号、图片排版之类的问题常常让人心力憔悴。LaTeX模板的出现就是将我们从诸如此类的体力劳动上解放出来，将注意力放在论文内容上。<br><a id="more"></a><br>不用担心，虽然它听起来比较陌生，但你只需要花上几个小时就可以掌握它90%的用法，节省你未来几十个小时的体力劳动时间。本人研究生学位论文使用LaTeX模板编写，经历了查重、提交学院、提交图书馆和打印过程，实践证明各个流程都无障碍，体验顺滑。</p><h2 id="1-TeX、LaTeX、TeX-Live"><a href="#1-TeX、LaTeX、TeX-Live" class="headerlink" title="1 TeX、LaTeX、TeX Live"></a>1 TeX、LaTeX、TeX Live</h2><p>使用之前我们先花几分钟了解一些概念，这将有助于提升你对这个模板的掌控感。</p><p><strong>TeX</strong>是一个引擎。它定义了命令与文档格式的对应关系，其作用是将用户输入的命令生成格式化的文档。（TeX引擎有：Knuth TeX、e-TeX、pdfTeX、LuaTeX、XeTeX、pTeX、upTeX、e-upTeX、pTeX-ng，引擎的概念可类比C语言的gcc）</p><p><strong>LaTeX</strong>是一种宏语言。它将TeX的某些命令封装成一种宏格式，用户不需要输入TeX的底层命令，只需输入按照这种宏语言输入宏。LaTeX将宏解释成TeX命令，然后交给TeX引擎生成格式化的文档。（其他宏语言有：plain TeX，LaTeX是当今使用最为广泛的宏语言。）</p><p><strong>TeX Live</strong>是TeX的发行版本。它包含了若干TeX引擎和若干宏语言，同时提供一个命令编辑器（TeX works），使得用户可以方便完成编辑命令、选择宏语言、调用引擎生成文档的流程。（其他TeX发行版本有：TeXStudio）</p><p>引擎和宏语言组合成一种格式，生成文档前，我们需要告诉软件我在使用什么格式。常见的格式有</p><p>（引擎 + 宏语言 &gt; 格式）</p><ul><li>e-TeX + LaTeX &gt; LaTeX</li><li>pdfTeX + LaTeX  &gt; pdfLaTeX</li><li>LuaTeX + LaTeX  &gt; LuaLaTeX</li><li>XeTeX + LaTeX  &gt; XeLaTeX</li><li>pTeX + LaTeX  &gt; pLaTeX</li><li>upTeX + LaTeX  &gt; upLaTeX</li><li>e-upTeX + LaTeX  &gt; e-upLaTeX</li><li>pTeX-ng + LaTeX  &gt; pLaTeX-ng</li></ul><h2 id="2-NUA2-Thesis"><a href="#2-NUA2-Thesis" class="headerlink" title="2 NUA2 Thesis"></a>2 NUA2 Thesis</h2><p>基于上述的引擎、宏语言等，南航民间校友开发了一套模板：<a href="https://github.com/nuaatug/nuaathesis" target="_blank" rel="noopener">南京航空航天大学(非官方)学位论文 LaTeX 模板</a>（下面简称为<strong>NUA2 Thesis</strong>），开源发布在Github上。据悉是由几名南航校友2017年左右费劲力气倒腾出来的，模板参照南航官方的<a href="http://aao.nuaa.edu.cn/2016/1222/c8098a170586/page.psp" target="_blank" rel="noopener">《本科毕业设计（论文）撰写格式要求》</a>和<a href="http://www.graduate.nuaa.edu.cn/2011/0831/c2122a53807/page.htm" target="_blank" rel="noopener">《南京航空航天大学研究生学位论文撰写要求》（2015年4月修订）</a>制作。</p><p>前人栽树，后人乘凉。感谢校友们的辛勤劳动~</p><h2 id="3-生成你的第一篇文档"><a href="#3-生成你的第一篇文档" class="headerlink" title="3 生成你的第一篇文档"></a>3 生成你的第一篇文档</h2><p>准备工作：</p><ul><li>下载并安装<a href="https://www.tug.org/texlive/acquire-netinstall.html" target="_blank" rel="noopener">TeX Live</a></li><li>下载并安装<a href="https://www.xm1math.net/texmaker/" target="_blank" rel="noopener">Texmaker</a></li><li>下载<a href="https://github.com/nuaatug/nuaathesis/releases" target="_blank" rel="noopener">NUA2 Thesis</a></li></ul><p>根据你的网络状态，这两个工作可能花费几分钟到几个小时的时间，不要着急，期间你可以去做其他事情。</p><p>NUA2 Thesis文件夹内部有三个文件夹demo_chs、demo_en和demo_ja，一般我们都是使用中文写论文，所以我们只用关心demo_chs这个文件夹里面的内容：</p><ul><li>\bib：存放参考文献目录</li><li>\content：存放子章节</li><li>\fig：存放论文图</li><li>*.tex：论文源文件</li><li>*.pdf：最终生成的文档</li><li>*.bst：格式定义</li><li>*.cls：格式定义</li></ul><p>.tex文件是论文编辑的源文件，我们在.tex文件中输入论文，TeX引擎会将.tex文件渲染成最终的pdf文档。</p><p>用Texmaker打开master.tex文件，master.tex相当于C语言里的main()函数。我们先不细究这个文件里面各行代码的意思，而选用一种TeX引擎渲染master.tex来生成文档获得直观的了解。</p><p>按照NUA2 Thesis的指导，我们（1）使用<code>XeLaTeX</code>格式，（2）点击<code>Run</code>，（3）运行十几秒之后，提示无报错就说明生成成功，（4）选择<code>View pdf</code>，（5）点击<code>View</code>就可以在右侧看到生成的pdf文档了。图解如下：</p><p><img src="L1-1.jpg" alt=""><br><img src="L1-2.jpg" alt=""><br><img src="L1-3.jpg" alt=""><br><img src="L1-4.jpg" alt=""><br><img src="L1-5.jpg" alt=""><br><img src="L1-6.jpg" alt=""></p><p>至此验证完毕：LaTeX软件安装正常、模板运行正常，接下来要做的就是往这个模板里填充内容，正式开始写论文啦。</p>]]></content>
      
      
      
        <tags>
            
            <tag> NUA2 Thesis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一些好玩的东西 | Grammarly、防疫末期的武汉街头</title>
      <link href="/2020/04/30/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%8E%A9%E7%9A%84%E4%B8%9C%E8%A5%BF%20-%20Grammarly%E3%80%81%E9%98%B2%E7%96%AB%E6%9C%AB%E6%9C%9F%E7%9A%84%E6%AD%A6%E6%B1%89%E8%A1%97%E5%A4%B4/"/>
      <url>/2020/04/30/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%8E%A9%E7%9A%84%E4%B8%9C%E8%A5%BF%20-%20Grammarly%E3%80%81%E9%98%B2%E7%96%AB%E6%9C%AB%E6%9C%9F%E7%9A%84%E6%AD%A6%E6%B1%89%E8%A1%97%E5%A4%B4/</url>
      
        <content type="html"><![CDATA[<p>&emsp;<br><a id="more"></a></p><h4 id="好玩的工具"><a href="#好玩的工具" class="headerlink" title="好玩的工具"></a>好玩的工具</h4><ul><li><p>Grammarly, PowerWritingAid and Microsoft Word<br>  少数派的文章 <a href="https://sspai.com/post/59984" target="_blank" rel="noopener">我的英语智能写作助手，是否真的「智能」？</a> 和 <a href="https://sspai.com/post/60185" target="_blank" rel="noopener">Microsoft Editor vs. Grammarly: 谁更适合你？</a> 介绍 PowerWritingAid 和 Grammarly 的使用体验对比，正好今天需要检查一下论文的摘要，我就试用了一下这三个软件的效果：Grammarly 免费版，PowerWritingAid 免费版，Microsoft Word。三个软件都能做的有单词拼写检查、标点使用检查，PowerWritingAid 免费版可以在这个基础上进行句式分析，比如检查出被动句并建议优化成主动语态，还可以检查副词并建议删除。显然这些建议不符合科技文的表达风格，于是这个功能对我来说没有什么用处。PowerWritingAid 免费版能做到的 Grammarly 免费版也可以做到，此外 Grammarly 免费版可以检查定冠词使用，比如缺定冠词或者定冠词用错。<br>  <img src="article.png" alt="定冠词检查"></p>  <center><font size="1"> ▲ 缺少定冠词</font> </center><p>  <img src="a-an.png" alt="定冠词搭配不当"></p>  <center><font size="1"> ▲ 定冠词搭配不当</font> </center></li><li><p>PowerToys<br>  微软官方的提升 Windows 使用体验的小工具软件合集 <a href="https://github.com/microsoft/PowerToys" target="_blank" rel="noopener">PowerToys</a>，功能有文件批量重命名、图像修改尺寸，我觉得比较有趣的是快捷键导航和窗口自动布局功能。快捷键导航是长按<code>Windows</code>键，屏幕会弹出快捷键组合。我 Windows 组合快捷键使用比较频繁，但用这个功能来了解一些冷门的快捷键很不错。窗口自动布局功能是预先设置一个窗口布局，比如左右分栏。之后按住<code>Shift</code>键将当前窗口拖进分配好的区域即可。下面贴一个快捷键指导的使用过程截图。<br>  <img src="powertoys.png" alt="快捷键导航使用过程"></p>  <center><font size="1"> ▲ 快捷键导航使用过程</font> </center></li><li><p>Size.link<br>  如果你在网上购买某个尺寸的物品的同时又不太能感知这种尺寸拿到手后具体有多大，那么进入这个网站可以帮助到你。进入网页，输入长、宽、高，就可以通过AR在你的桌子或者地板上看这个东西具体有多大。网址：<a href="https://size.link" target="_blank" rel="noopener">size.link</a><br>  <img src="size.link.jpg" alt="地板上显示出立方体"></p>  <center><font size="1"> ▲ 地板上显示出立方体</font> </center></li></ul><h4 id="人类观察"><a href="#人类观察" class="headerlink" title="人类观察"></a>人类观察</h4><p><img src="matou.jpg" alt="中华路码头"></p><center><font size="1"> ▲ 中华路码头，乘客乘坐前需要扫码、量体温（2020年4月16日，武汉市武昌区街头）</font> </center><p><img src="zhijing.jpg" alt="致敬白衣天使"></p><center><font size="1"> ▲ 「致敬白衣天使」（2020年4月16日，武汉市武昌区街头）</font> </center><p><img src="cunzi.jpg" alt="村子"></p><center><font size="1"> ▲ 在老家躲避了两个多月的瘟疫：起点时刻和终点时刻 </font> </center><p><img src="liaotian.jpg" alt="启动尬聊"></p><center><font size="1"> ▲ 启动尬聊 </font> </center>]]></content>
      
      
      
        <tags>
            
            <tag> 一些好玩的东西 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch | 使用GPU进行模型训练</title>
      <link href="/2020/04/18/PyTorch%20-%20%E4%BD%BF%E7%94%A8GPU%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
      <url>/2020/04/18/PyTorch%20-%20%E4%BD%BF%E7%94%A8GPU%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<p>使用NVIDIA GPU进行运算，与使用CPU计算相比，算法代码需要进行改动的有</p><ul><li>模型的输入数据需要转换成GPU支持的数据类型（使用<code>.to(device)</code>）</li><li>模型需要转换成GPU支持的数据类型（使用<code>.to(device)</code>）</li><li>代价函数转换成GPU支持的数据类型（使用<code>.to(device)</code>）</li></ul><a id="more"></a><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#%% 1.Loading and normalizing CIFAR10</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>,</span><br><span class="line">                                        download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>,</span><br><span class="line">                                       download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="keyword">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># show some of the training images</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br><span class="line">print(<span class="string">"Data Imported."</span>)</span><br></pre></td></tr></table></figure><h4 id="定义神经网络模型"><a href="#定义神经网络模型" class="headerlink" title="定义神经网络模型"></a>定义神经网络模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 2.Define a Convolutional Neural Network</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(<span class="string">"Model Created."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#%% Use GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">  <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">  net = nn.DataParallel(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.to(device)  <span class="comment"># to GPU</span></span><br><span class="line">print(<span class="string">"Model to GPU."</span>)</span><br></pre></td></tr></table></figure><h4 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 3. Define a Loss function and optimizer</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># to GPU</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">print(<span class="string">"Loss and Optimizer Function Defined."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># to GPU</span></span><br><span class="line">criterion.to(device)</span><br><span class="line">print(<span class="string">"Loss Function To GPU."</span>)</span><br></pre></td></tr></table></figure><h4 id="训练神经网络模型"><a href="#训练神经网络模型" class="headerlink" title="训练神经网络模型"></a>训练神经网络模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 4. Train the network</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># to GPU</span></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save our trained model</span></span><br><span class="line">PATH = <span class="string">'./cifar_net.pth'</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure><h4 id="验证模型"><a href="#验证模型" class="headerlink" title="验证模型"></a>验证模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%% 5. Test the network on the test data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load back in our saved model</span></span><br><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">outputs = net(images)</span><br><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><h4 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h4><p>运行程序，输出：<br><img src="gpuresult.png" alt="输出"><br>查看GPU状态，确认使用了GPU：</p><p><img src="running.png" alt="训练中"></p><center><font size="1"> ▲ 训练中</font> </center><p><img src="pending.png" alt="训练结束"></p><center><font size="1"> ▲ 训练结束</font> </center>]]></content>
      
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch | 线性回归模型训练</title>
      <link href="/2020/04/18/PyTorch%20-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
      <url>/2020/04/18/PyTorch%20-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<p>PyTorch 的 HelloWorld 工程：以编写简单的线性回归模型为例，来理解PyTorch的各个模块的使用方法。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Training Dataset</span></span><br><span class="line">data_x = torch.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">data_y = torch.linspace(<span class="number">2</span>,<span class="number">11</span>,<span class="number">10</span>)</span><br><span class="line">plt.scatter(data_x.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),data_y.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),label = <span class="string">'data'</span>,color = <span class="string">'red'</span>)</span><br><span class="line">myDataSet = Data.TensorDataset(data_x,data_y)</span><br><span class="line">myDataLoader = Data.DataLoader(dataset = myDataSet,batch_size = <span class="number">1</span>,shuffle = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Linaer Regression Model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Linear,self).__init__()</span><br><span class="line">        self.f1 = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.f1(x)        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Model</span></span><br><span class="line">net = Linear()</span><br><span class="line">criterion = nn.L1Loss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(),lr = <span class="number">0.01</span>, momentum = <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train Model</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">60</span>):</span><br><span class="line">    <span class="keyword">for</span> i,data <span class="keyword">in</span> enumerate(myDataLoader,<span class="number">0</span>):</span><br><span class="line">        x,y_target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        y_hat = net(x)</span><br><span class="line">        loss = criterion(y_hat,y_target)</span><br><span class="line">        loss.backward()    </span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">19</span> == <span class="number">0</span>:</span><br><span class="line">        y_hat = net.state_dict()[<span class="string">'f1.weight'</span>] * data_x + net.state_dict()[<span class="string">'f1.bias'</span>]</span><br><span class="line">        plt.plot(data_x.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),y_hat.view(<span class="number">-1</span>,<span class="number">1</span>).numpy(),label = <span class="string">'prediction'</span>)</span><br><span class="line">        <span class="comment"># plt.legend()</span></span><br></pre></td></tr></table></figure></p><p>数据拟合结果：<br><img src="linear_regression.png" alt="线性回归"></p>]]></content>
      
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一些好玩的东西 | Robotics在线课程</title>
      <link href="/2020/04/10/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%8E%A9%E7%9A%84%E4%B8%9C%E8%A5%BF%20-%20Robotics%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0/"/>
      <url>/2020/04/10/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%8E%A9%E7%9A%84%E4%B8%9C%E8%A5%BF%20-%20Robotics%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>&emsp;<br><a id="more"></a></p><h4 id="好玩的工具"><a href="#好玩的工具" class="headerlink" title="好玩的工具"></a>好玩的工具</h4><ul><li><p>桌面动态画面截图<br>  ScreenToGif，Github开源项目，功能齐全，操作顺手，很方便将桌面的动态内容录制成gif导出分享。</p></li><li><p>图床<br>  七牛云的口碑比较不错，可以当做静态图和动态图的图床。缺点是想要使用免费的图床空间，我需要上传自己的身份证进行实名认证。<del>这个博客上的所有图片都是存在七牛云图床上的</del>30天之后我发现：七牛云要求将账户的储存空间挂靠在自己的已备案的域名下，而Github Page不属于已备案的站点，我无法进行挂靠，罢了，我直接将博客的图片存在Github Page上。</p></li></ul><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><ul><li>Robotics在线课程<br>  用前两个工具录制我完成课程编程任务的演示结果并储存在图床中。<br>  <img src="perception_e1.gif" alt="Dolly Zoom Effect">  <center><font size="1"> ▲ Dolly Zoom Effect</font> </center><br>  <img src="perception_e2.gif" alt="Image Projection"><br>  <center><font size="1"> ▲ Image Projection</font> </center><br>  <img src="perception_e3.gif" alt="AR"><br>  <center><font size="1"> ▲ AR</font> </center><br>  <img src="perception_e4.gif" alt="Structure from Motion"><br>  <center><font size="1"> ▲ Structure from Motion</font> </center></li></ul><h4 id="人类观察"><a href="#人类观察" class="headerlink" title="人类观察"></a>人类观察</h4><p><img src="shequlouxia.png" alt="劫后余生"></p><center><font size="1"> ▲ 劫后余生（2020年4月6日，武汉市武昌区街头）</font> </center>]]></content>
      
      
      
        <tags>
            
            <tag> 一些好玩的东西 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>雅思考试回顾总结</title>
      <link href="/2019/11/01/%E9%9B%85%E6%80%9D%E8%80%83%E8%AF%95%E5%9B%9E%E9%A1%BE%E6%80%BB%E7%BB%93/"/>
      <url>/2019/11/01/%E9%9B%85%E6%80%9D%E8%80%83%E8%AF%95%E5%9B%9E%E9%A1%BE%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>英语基础：CET4-591，CET6-520<br>第一次考雅思2019年4月6日，纸笔（南京），复习15天，成绩：Overall 6.5 (L7.5，R6.5，W5.5，S6.0)<br>第二次考雅思2019年9月10日，机试（上海），复习30天，成绩：Overall 7.0 (L7.5，R7.0，W6.5，S6.5)<br><a id="more"></a></p><h4 id="听"><a href="#听" class="headerlink" title="听"></a>听</h4><p>听力与口语息息相关，心中熟悉音标、嘴上发音标准，这样才能在做听力时快速地将听到的单词发音与脑子里的单词意义对应，考口语时让考官没有障碍地理解你说的每个单词。我是通过看YouTube频道上的《谢丽媛发音篇》》来逐个学习音标、纠正自己以前未注意到的发音误区。</p><p>针对雅思的听力我没有进行特别的训练，主要依赖于日常听英文播客或者视频栏目，约每天30分钟。一方面是为了备考雅思，另一方面也是因为自己本身对栏目的兴趣。栏目如下</p><p>播客栏目</p><p>99% Invisible：人文类，讲述世界多种地区发展的故事。主题区分明显，因此生词可能比较多。但是官网上有对应的原稿，听完一遍之后看一遍原稿，重点听之前没有听清楚的地方。或者第一次听之前，像预习课文一下读一遍原稿，心里大致知道会讲哪些东西。熟练之后就可以逐渐不用看原文也可以听懂了。</p><p>Intelligence Square Debate U.S.：辩论节目，议题主要涉及政治政策。官网有原稿，语速正常，吐词清晰。适合对辩论或者时事感兴趣的学习者。</p><p>YouTube频道</p><p>TED：个人演讲，包罗万象——教育、设计、科学研究、人文探索，常会感叹「原来有人在研究这个」。刚开始可能难以跟上时可以在不懂的地方打开字幕，但是提升听力能力建议一定要关掉字幕观看。</p><p>Vox：科普/新闻，讲历史、科技、发展等。</p><p>Vice News：新闻频道，每天报道世界各地事件。</p><p>The Verge：科技媒体，手机、电脑等电子设备测评。</p><p>Great Big Story：制作精美的小故事，讲述不同地区的风俗人文。</p><p>storybooth：读者投稿类故事节目，节目组添加动画形成视频。由于是读者自己朗读自己的故事，所以难免会有一定口音。但是我觉得这个里面的词语和句式很口语化，适合日常对话中使用，因此我是跟读每个视频尝试熟练背一两句句式，丰富自己的口语语料。</p><p>针对雅思考试，我觉得《雅思王听力真题语料库》很好用，听这个可以很好的训练发音与单词的快速对应能力，还可以检查自己单词的发音是否准确。临近考试之前最好的训练方法是做剑桥试题。</p><h4 id="说"><a href="#说" class="headerlink" title="说"></a>说</h4><p>说分为两个部分：说的准确和有话说。说的准确是指发音标准、语调自然连贯，这一部分通过模仿与纠正来实现；有话说是指针对某一主题，脑子里有足够多的相关的单词、句式来支撑你表达自己想法。比如设想在学术会议上交流的场景，当讲者做完报告之后，我需要在脑子里整理自己的问题、搜索这个学科的相关词汇、提问句式，最后将问题清晰地表达出来。</p><p>在学习发音之前，我觉得应该先区分英音和美音。虽然九年义务教育和高中大学英语教育没有重视过区分英音和美音的不同，但是它们的差异不小。比如「mask」，英音读/mɑːsk/，美音读/mæsk/，混用英音和美音对做听力影响很大。通过前面提到的《谢丽媛发音篇》，我觉得可以解决绝大部分发音不标准的问题。此外碰到不熟悉或者发音没有把握的单词就查字典，逐渐将音标与单词一一对应。</p><p>另一方面是语调问题，正常的英语交流是抑扬顿挫的。前面提到的播客栏目和YouTube频道，选取自己感兴趣的部分模仿与跟读。拿手机录下自己的声音，然后回放。一遍遍纠正之后，你会逐渐形成新的语调。进行这个步骤的学习时，不妨「矫枉过正」，故意夸张地放大一句话里面的升调和降调。因为其实真正与人实景交流时，由于害羞、或者担心自己的语调不对，人难免不会像自己练习时那样升降明显。</p><p>针对「有话说」这个方向的积累，我觉得比较好的单人训练的方法是给自己随机定一个主题，然后计时五分钟，强迫自己说满这五分钟。事后听录音时你会发现自己词汇量和句式的贫乏，可能是重复说一些东西。然后就上网搜索这个主题——用英文搜索，把自己设想成母语为英语，积累单词和表达。每天一个主题，一年的积累量将是可观的。</p><p>此外为了使学习过程有趣一点，可以关注学校里面的语言相关的活动。比如国际教育学院的一些活动需要招募语言类志愿者，我参加过一次Summer School，全程与外国学生交流，验证自己阶段学习成果，提升自己的自信心。</p><p>请外教也是个短期提升对话能力的一个好办法。目前（2019年8月）市场价格约1.5~2.0元/分钟。我在考雅思之前使用伴鱼app，上面有来自各个国家的职业或业余英语老师提供一对一口语辅导。然而，我觉得请外教主要用于提升自己的音标、语调和语感，并不能短期内丰富你自己的口语语料库——即你不知道说什么最终还是不知道说什么。所以建议在词汇量和句式库积累到一定程度后，再请外教来提升。</p><p>针对雅思考试，《慎小嶷——十天突破雅思口语》够用，适合目标为6.5分的人群。</p><h4 id="读"><a href="#读" class="headerlink" title="读"></a>读</h4><p>读对于大多数人来说应该不在话下，掌握足够的词汇量和扎实的语法基础即可：背单词是为了认识它，学语法是为了看懂句子。所以这一步的重点是背单词和学语法。</p><p>背单词我推荐使用欧陆辞典，它有两个优点：一、可以导入<a href="https://pan.baidu.com/s/1C8kYr0Rbfxb7OYb--KmrDA" target="_blank" rel="noopener">辞典库</a>，比如牛津高阶辞典和搭配辞典；二、背单词的卡片上显示的即是专业辞典的释义，而不是想百词斩一样的图片，这样最终记住的是释义而不是图片。而且具备移动端和桌面端之间的同步功能。</p><p>学语法我暂时没有好的学习资料，依靠自己高中的英语积累。</p><p>掌握了词汇量和语法之后，需要日常不断的练习，比如阅读英语报刊、书籍、文献等。这里我推荐的报刊有：</p><p>The Economist：英国媒体，周刊，涉及政治、文化、科技。逻辑完整、用词考究，既适合用来练习阅读，也适合用来模仿写作。</p><p>SCMP：South China Morning Post，香港媒体，日报，这个用来了解新闻、保持日常阅读量即可，不建议模仿新闻报道写作。</p><p>针对雅思考试，我没有进行单独的训练，只在考试之前熟悉了模拟试题上各个部分的用时分配。</p><h4 id="写"><a href="#写" class="headerlink" title="写"></a>写</h4><p>写是我最不擅长的一个方面，因为日常几乎没有什么工作是涉及到英语写作的。我第一次考雅思时信心满满，以为跟高考英语作文差不多，结果滑铁卢。第二次认认真真地啃《慎小嶷——十天突破IELTS写作完整真题库与6-9分范文全解》，背诵范文结构与句式，模拟写作约四十篇。第二次发挥就好多了。</p><p>资源下载</p><hr><ol><li><a href="https://pan.baidu.com/s/1C8kYr0Rbfxb7OYb--KmrDA#list/path=%2F" target="_blank" rel="noopener">欧陆辞典词库</a></li><li><a href="https://pan.baidu.com/s/1aukZ9C57ffAPjQ2IeywdxA#list/path=%2F" target="_blank" rel="noopener">《谢丽媛发音篇》</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 雅思 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018 年北京驱逐「低端人口」事件</title>
      <link href="/2018/09/16/2018%E5%B9%B4%E5%8C%97%E4%BA%AC%E9%A9%B1%E9%80%90%E3%80%8C%E4%BD%8E%E7%AB%AF%E4%BA%BA%E5%8F%A3%E3%80%8D%E4%BA%8B%E4%BB%B6/"/>
      <url>/2018/09/16/2018%E5%B9%B4%E5%8C%97%E4%BA%AC%E9%A9%B1%E9%80%90%E3%80%8C%E4%BD%8E%E7%AB%AF%E4%BA%BA%E5%8F%A3%E3%80%8D%E4%BA%8B%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>2017 年 11 月 18 号，北京市大兴区的一栋群租住房发生重大火灾事故，19 人死亡，8 人受伤。</p><p>大火导致北京市内燃起为期 40 天的「安全隐患大排查、大清理、大整治专项行动」，聚居在市内出租公寓、工业园区等地的大量外来人口被迫搬迁。与此次大火一同引发民众关注的，还有「低端人口」一词。虽然官方急于与该词撇清关系，网民们仍利用网络搜索引擎查到了北京市多个地区政府公文中出现「低端人口」。<br><a id="more"></a><br>我未想到政府会如此毫不留情地在冬日的寒风中驱赶他的公民，更难以想象政府会如此坦荡地用「低端人口」来称呼自己的同胞。</p><h4 id="溯源"><a href="#溯源" class="headerlink" title="溯源"></a>溯源</h4><p>2002 年，申奥成功的北京因为奥运设施兴建以及城市风貌整改等计划，大批非京籍人口开始涌入。北京市人口不断增加，城市空气污染问题、交通堵塞问题越来越严重，北京市开始制定多种控制人口相关政策：</p><p>2005 年《北京城市总体规划（2004-2020）》指出，2020 年北京市总人口规模计划控制在 1800 万左右。</p><p>2006 年《北京市“十一五”时期重点新城发展实施规划》中，其中「完善人口发展相关政策」一节提出：通过产业结构调整、减少低端产业、强化外来人口管理、加强出租房屋管理、拆除违法建设和改造「城中村」，以及加强本地人口尤其是农业人口的就业培训、强化城市管理等综合调控与管理手段，防止低端外来人口在新城大规模聚集，进一步加大高端人才的引进力度，提高新城人口素质和优化人口结构。这也是「低端人口」这一词较早的出现源头。</p><p>2010 年，北京市政府工作报告提出“通过城市功能疏解、产业结构升级和布局调整，促进人口有序迁移与合理分布。</p><p>2011 年 1 月，在北京市工商行政管理工作会议上，北京市政府表示要「屏蔽不符合首都功能定位的行业形态」，提出了「以业控人」的「加减法」规划，小百货等 17 类业态将提升审批准入门槛，约涉及 30 万户商业主体、100 万流动人口。同年 8 月，北京市政府公布《北京市人民防空工程和普通地下室安全使用管理办法》，禁止出租地下室。</p><p>2014 年，北京市政府首次提出人口增速明显下降的目标，并提高非京籍适龄儿童入读公办小学门槛。</p><p>2015 年，北京市政府提出疏解北京非首都功能，包括构建「高精尖经济结构」和空间结构等。</p><p>2016 年，北京提出的常住人口控制目标是 2300 万；下半年，北京居住证制度计划正式实施，确定了严格的落户门槛。</p><p>2017 年年初，北京开展「疏解整治促提升」专项行动，主要包括拆除违法建设、整治「开墙打洞」、城乡结合部整治改造等十个方向，并设定目标值。</p><p>北京不欢迎你。</p><p>2017 年 11 月 18 日，北京市大兴区的一栋群租住房发生重大火灾事故，造成 19 人死亡，8 人受伤。旋即政府以「安全清查」为名，采取断水断电、断供暖、断燃气、限期搬家、强行清理、行政拘留等手段，驱逐了北京市 13 个区的约 292 处居住点，涉及群众近十万人。无辜的租客未得到任何房租补偿以及后期安置，在北京的寒风中匆匆寻找下一处落脚点。</p><h4 id="发问"><a href="#发问" class="headerlink" title="发问"></a>发问</h4><p>人权、法治、言论——在此次「2017 年 11 月北京驱逐「低端人口」」事件中被踩在地上蹂躏。</p><p>人之所以比动物高级，是因为我们具有同情之心。而此次事件中，北京政府对待自己的同胞却不见有丝毫的同情心，执法人员视上级行政命令大于天，在寒冷的冬日粗暴地驱逐无辜民众至落难街头。</p><p>此次驱逐行动程序合法吗？本次驱逐行动缺乏公众参与、缺乏专家论证、缺乏风险评估、缺乏合法性审查、缺乏集体讨论，如此草率的行动的程序正义何在？另一方面，虽然清理“违章建筑”有法理依据，但处理的对象应是房主而非租客。现在北京市却让租客去承受所有恶果，更不用提「补偿」租客财产与精神损失的问题。此二者严重违背  「依法治国」的理念。</p><p>事发之后，北京政府一如既往地控制网上言论。社交媒体上与驱逐事件有关的博文或资讯遭到封杀，百名知识分子联名上书的新闻遭到禁言，民间团体自发援助受灾民众却因为「众所周知的原因」被迫中止。何以使政府对舆论如此担惊受怕？我不得而知。</p><p>一场驱逐事件，将我国长期存在的矛盾暴露无遗。</p><p>在资源分配不均的情况下，人自然会向资源丰富的地区聚集，如同动物为了适应环境而迁徙一般，人也追求更好的生活。中国从 1968 年开始实行的户籍制度和人事档案制度，户籍制度将公民划分成「城市户口」和「农村户口」，旧时依据不同户口类别分配生活物资，而直到当今，不同地区或不同类别的户口在教育、医疗、社会福利等方面的仍差别悬殊；人事档案制度要求用人单位保留员工个人档案，员工入职新单位时须经原单位同意后迁出、经新单位同意后迁入个人档案。</p><p>户籍制度和人事档案制度如同两座大山，横在每一个渴望自由迁徙的人前。若要使各地区都发展均衡实属难题，那么以户籍制度和人事档案制度简单地限制人口的流动，则是执政者思想上的懒惰和既得利益者自私自利的本质体现。</p><p>除此之外， 立法、司法制度的落后亦在此次事件中尽显。立法过程是否合法？涉及基层民生的法律和政策却不是由基层民众参与制定或是讨论；执法过程是否合法？根据网上的视频，手持警棍的执法人员踹开出租屋、收缴私人物品，官方如此的行为着实令人不安。数万甚至数十万「低端人口」在数天之内被通知「安全大清查」，被强行要求离开住处。他们事先不知情，事情发生时无处发声，事后申诉、索赔无门。因为一纸通知就是大于天的王法，「低端人口」丝毫没有说「不」的权利。</p><p>2003 年，大学生孙志刚在收容所内身亡，当时事件尚因有南方都市报的记者深入跟踪报道而引起全国广泛关注，并直接推动废除实行多年的“收容制度”。而在此次驱逐事件中，已经难以看到有媒体跟进，更不用说进行深入调查反思事件的起因和制度缺漏，进而推动制度改革。自由媒体之死，亦即千万百姓之哀。</p><h4 id="尊严"><a href="#尊严" class="headerlink" title="尊严"></a>尊严</h4><p>我家附近可以碰到形形色色的人，清理街道的环卫工，早点摊位上卖热干面的大妈，夏天卖啤酒的冷饮店老板……按照北京政府的定义，社会底层的朴实众生有几个不是「低端人口」？工作环境恶劣、工作时间长、职业不被人们尊重等因素是客观环境，但主观上他们和千百万同胞一样，无论「低端」与「高端」，都是憧憬生活，充满希望；辛勤耕耘，朝夕奔梦。</p><p>人们常常以人口对经济的贡献来衡量人的价值，因此各地落户政策设置门槛，高素质人才可以撑起本地的经济繁荣，因此他们被接纳。貌似合情合理的逻辑即是问题的根源：从何时起，人被作为一种资本，而一个人的价值也被等同于他或她所能生产出的经济效益？</p><p>社会各式各样的制度赋予了人不同的身份和标识，以此提供了人群互相歧视的场域。但愿每个人都不迷失在既得利益的熏陶中，仍视人为鲜活的个体，而不是表格上代表劳动力与生产力的一个个数据点。我们有所思、有所求，我们都需要独立而有尊严的活着。</p><p>引用标注：<br>本文引用的数据及相关政策来自端传媒、维基百科。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 低端人口 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
